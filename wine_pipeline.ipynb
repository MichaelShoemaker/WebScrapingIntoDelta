{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "applied-robertson",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from bs4 import BeautifulSoup, NavigableString, Tag\n",
    "\n",
    "URL = \"https://thepurplepigchicago.com/drink\"\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "menu = soup.find_all(\"div\", class_=\"menu-section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "liable-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wines_df():\n",
    "    sections = []\n",
    "    items = []\n",
    "    staging = []\n",
    "    def clean_extra(extra):\n",
    "        if extra == None:\n",
    "            return ''\n",
    "        else:\n",
    "            return ' '.join(x.strip() for x in extra.replace('\\n','').split('/'))\n",
    "\n",
    "    for i in menu:\n",
    "        sections.append((i.find('div',{'class':'menu-section-title'}).text, \\\n",
    "                             zip([x.text.split('|')[1] if '|' in x.text else x.text for x in i.find_all('div',{'class':'menu-item-title'})], \\\n",
    "                            [x.text.replace('|','') for x in i.find_all(\"div\", class_=\"menu-item-description\") if x != None], \\\n",
    "                            [clean_extra(x.text) for x in i.find_all(\"div\", class_=\"menu-item-price-bottom\")])))\n",
    "\n",
    "\n",
    "\n",
    "    for i in sections:\n",
    "        category = i[0]\n",
    "        for items in i[1]:\n",
    "            staging.append([\"PurplePig\"]+[category] + [x for x in items])\n",
    "\n",
    "    dict_holder = []\n",
    "    key_list = ['origin','section', 'name', 'description','extra']\n",
    "\n",
    "    for i in staging:\n",
    "        dict_from_list = dict(zip(key_list, i))\n",
    "        dict_holder.append(dict_from_list)\n",
    "    return(pd.DataFrame(dict_holder).apply(lambda x: x.str.strip() if x.dtype == \"object\" else x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "58dff4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_wines_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "015af4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['insert_date'] = pd.to_datetime('today').normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "824f7346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 544 entries, 0 to 543\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   origin       544 non-null    object        \n",
      " 1   section      544 non-null    object        \n",
      " 2   name         544 non-null    object        \n",
      " 3   description  544 non-null    object        \n",
      " 4   extra        544 non-null    object        \n",
      " 5   insert_date  544 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](1), object(5)\n",
      "memory usage: 25.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cf76dd1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-b78153a89288>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'insert_date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df['insert_date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "peaceful-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('Wines.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "forced-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config('spark.ui.port', '4050') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "returning-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('Wines.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6e477262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|   origin|             section|                name|         description|               extra|        insert_date|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "|PurplePig|SOMMELIER WINE SP...|     SAUVIGNON BLANC|Patrick Noël, San...|         $19 $45 $86|2022-06-09 19:00:00|\n",
      "|PurplePig|WHISKEY OF THE MONTH|NELSON'S \"GREEN B...|                  14|Nose: Full and wa...|2022-06-09 19:00:00|\n",
      "|PurplePig|SPARKLING BY THE ...|CHAMPAGNE (CHARDO...|Drappier, Carte d...|        $22 $53 $101|2022-06-09 19:00:00|\n",
      "|PurplePig|SPARKLING BY THE ...|           LAMBRUSCO|Carra di Casatico...|         $13 $32 $61|2022-06-09 19:00:00|\n",
      "|PurplePig|SPARKLING BY THE ...|           XINOMAVRO|Kir-Yianni  Akaki...|         $16 $39 $74|2022-06-09 19:00:00|\n",
      "|PurplePig|SPARKLING BY THE ...|    PROSECCO (GLERA)|Pitars, Brut, Fri...|         $15 $36 $68|2022-06-09 19:00:00|\n",
      "|PurplePig|SPARKLING BY THE ...|CAVA ( MACABEO, X...|Cellers de Can Su...|         $15 $36 $68|2022-06-09 19:00:00|\n",
      "|PurplePig|SPARKLING BY THE ...|             MOSCATO|Bera, Moscato d’A...|         $15 $36 $68|2022-06-09 19:00:00|\n",
      "|PurplePig|SPARKLING BY THE ...|          PINOT NOIR|Dopff & Irion Cre...|         $16 $39 $74|2022-06-09 19:00:00|\n",
      "|PurplePig|   ROSÉ BY THE GLASS|GRENACHE + CLAIRETTE|Chateau De Trinqu...|         $14 $33 $63|2022-06-09 19:00:00|\n",
      "|PurplePig|   ROSÉ BY THE GLASS|    SYRAH + CINSAULT|Domaine Rimbert “...|         $14 $33 $63|2022-06-09 19:00:00|\n",
      "|PurplePig|   ROSÉ BY THE GLASS|    GAGLIOPPO ROSATO|Bacco, Calabria, ...|         $13 $32 $61|2022-06-09 19:00:00|\n",
      "|PurplePig|   ROSÉ BY THE GLASS|            CARIGNAN|VillaViva, Cotes ...|         $11 $27 $51|2022-06-09 19:00:00|\n",
      "|PurplePig|   ROSÉ BY THE GLASS|SANGIOVESE Chille...|Field Recordings,...|         $14 $33 $63|2022-06-09 19:00:00|\n",
      "|PurplePig| WHITES BY THE GLASS|           TREBBIANO|Tiberio, Trebbian...|         $15 $36 $68|2022-06-09 19:00:00|\n",
      "|PurplePig| WHITES BY THE GLASS|             PICPOUL|Jadix, Picpoul de...|         $13 $32 $61|2022-06-09 19:00:00|\n",
      "|PurplePig| WHITES BY THE GLASS|CARRICANTE --SOMM...|Tenuta di Fessina...|         $14 $33 $63|2022-06-09 19:00:00|\n",
      "|PurplePig| WHITES BY THE GLASS|          CHARDONNAY|Eric Chevalier, D...|         $15 $36 $68|2022-06-09 19:00:00|\n",
      "|PurplePig| WHITES BY THE GLASS|TXAKOLINA (HONDAR...|Extebarria, Bengo...|         $16 $39 $74|2022-06-09 19:00:00|\n",
      "|PurplePig| WHITES BY THE GLASS|          FALANGHINA|Feudi di San Greg...|         $14 $33 $63|2022-06-09 19:00:00|\n",
      "+---------+--------------------+--------------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()#printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "editorial-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('delta').save('tmp/delta/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2c07077e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE wines USING DELTA LOCATION './tmp/delta'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d671ffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------+--------------------+-----------+--------------------+\n",
      "|   origin|             section|           name|         description|      extra|         insert_date|\n",
      "+---------+--------------------+---------------+--------------------+-----------+--------------------+\n",
      "|PurplePig|SOMMELIER WINE SP...|SAUVIGNON BLANC|Patrick Noël, San...|$19 $45 $86|2022-06-10 09:24:...|\n",
      "|PurplePig| WHITES BY THE GLASS|SAUVIGNON BLANC|Chateau La Rame, ...|$15 $36 $68|2022-06-10 09:24:...|\n",
      "+---------+--------------------+---------------+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM wines WHERE name like '%BLANC%' LIMIT 10;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b467c587",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "The specified schema does not match the existing schema at tmp/delta.\n\n== Specified ==\nroot\n-- origin: string (nullable = true)\n-- section: string (nullable = true)\n-- name: string (nullable = true)\n-- description: string (nullable = true)\n-- extra: string (nullable = true)\n-- date: date (nullable = true)\n\n\n== Existing ==\nroot\n-- origin: string (nullable = true)\n-- section: string (nullable = true)\n-- name: string (nullable = true)\n-- description: string (nullable = true)\n-- extra: string (nullable = true)\n-- insert_date: timestamp (nullable = true)\n\n\n== Differences==\n- Specified schema is missing field(s): insert_date\n- Specified schema has additional field(s): date\n\nIf your intention is to keep the existing schema, you can omit the\nschema from the create table command. Otherwise please ensure that\nthe schema matches.\n        ;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-c4d0d4ef1f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CREATE DATABASE IF NOT EXISTS WINES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m spark.sql('''\n\u001b[0m\u001b[1;32m      6\u001b[0m     CREATE TABLE IF NOT EXISTS WINES.PURPLE_PIG(\n\u001b[1;32m      7\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: The specified schema does not match the existing schema at tmp/delta.\n\n== Specified ==\nroot\n-- origin: string (nullable = true)\n-- section: string (nullable = true)\n-- name: string (nullable = true)\n-- description: string (nullable = true)\n-- extra: string (nullable = true)\n-- date: date (nullable = true)\n\n\n== Existing ==\nroot\n-- origin: string (nullable = true)\n-- section: string (nullable = true)\n-- name: string (nullable = true)\n-- description: string (nullable = true)\n-- extra: string (nullable = true)\n-- insert_date: timestamp (nullable = true)\n\n\n== Differences==\n- Specified schema is missing field(s): insert_date\n- Specified schema has additional field(s): date\n\nIf your intention is to keep the existing schema, you can omit the\nschema from the create table command. Otherwise please ensure that\nthe schema matches.\n        ;"
     ]
    }
   ],
   "source": [
    "OUTPUT_DELTA_PATH = './tmp/delta/'\n",
    "\n",
    "spark.sql('CREATE DATABASE IF NOT EXISTS WINES')\n",
    "\n",
    "spark.sql('''\n",
    "    CREATE TABLE IF NOT EXISTS WINES.PURPLE_PIG(\n",
    "        origin string\n",
    "        , section string\n",
    "        , name string\n",
    "        , description string\n",
    "        , extra string\n",
    "        , date date\n",
    "    ) USING DELTA\n",
    "    LOCATION \"{0}\"\n",
    "    '''.format(OUTPUT_DELTA_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7eb0dabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| split(extra,  , -1)|\n",
      "+--------------------+\n",
      "|     [$19, $45, $86]|\n",
      "|[Nose:, Full, and...|\n",
      "|    [$22, $53, $101]|\n",
      "|     [$13, $32, $61]|\n",
      "|     [$16, $39, $74]|\n",
      "|     [$15, $36, $68]|\n",
      "|     [$15, $36, $68]|\n",
      "|     [$15, $36, $68]|\n",
      "|     [$16, $39, $74]|\n",
      "|     [$14, $33, $63]|\n",
      "|     [$14, $33, $63]|\n",
      "|     [$13, $32, $61]|\n",
      "|     [$11, $27, $51]|\n",
      "|     [$14, $33, $63]|\n",
      "|     [$15, $36, $68]|\n",
      "|     [$13, $32, $61]|\n",
      "|     [$14, $33, $63]|\n",
      "|     [$15, $36, $68]|\n",
      "|     [$16, $39, $74]|\n",
      "|     [$14, $33, $63]|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT split(extra,\\' \\') FROM WINES.PURPLE_PIG').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7aa78c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file = spark.read.parquet('Wines.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f39d4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file.createOrReplaceTempView('wine_load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aad93858",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o233.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition, true, [id=#1344]\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#3631L])\n   +- *(1) ColumnarToRow\n      +- FileScan parquet wines.purple_pig[] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeLogFileIndex[file:/home/gary/WebScrapingIntoDelta/tmp/delta], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:116)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:232)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:399)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:390)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:485)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:139)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:137)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:154)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 44 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-6a7e803b4db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m spark.sql(\"\"\"SELECT count(1)\n\u001b[0m\u001b[1;32m      2\u001b[0m FROM WINES.PURPLE_PIG\"\"\").show()\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o233.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition, true, [id=#1344]\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#3631L])\n   +- *(1) ColumnarToRow\n      +- FileScan parquet wines.purple_pig[] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeLogFileIndex[file:/home/gary/WebScrapingIntoDelta/tmp/delta], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:316)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:434)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2697)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2904)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:337)\n\tat jdk.internal.reflect.GeneratedMethodAccessor145.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.base/java.lang.Thread.run(Thread.java:829)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:116)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1476)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:232)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:399)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:390)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:485)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:162)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:106)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:139)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:137)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:154)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 44 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(1)\n",
    "FROM WINES.PURPLE_PIG\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df4d7026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|hash(origin, section, name, description)|\n",
      "+----------------------------------------+\n",
      "|                              2061198233|\n",
      "|                              1527710402|\n",
      "|                              -632903228|\n",
      "|                              1261612108|\n",
      "|                                 2099280|\n",
      "|                             -2124324733|\n",
      "|                             -1568785439|\n",
      "|                             -1899088557|\n",
      "|                             -1618643588|\n",
      "|                             -1993399701|\n",
      "|                              -515037650|\n",
      "|                               -21673382|\n",
      "|                             -1172576607|\n",
      "|                               128985245|\n",
      "|                             -1285102648|\n",
      "|                              1795557171|\n",
      "|                              1636546046|\n",
      "|                               643601468|\n",
      "|                             -1081905292|\n",
      "|                              -444497963|\n",
      "+----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT hash(origin,section, name, description)\n",
    "FROM wine_load\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c984b3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"MERGE INTO WINES.PURPLE_PIG\n",
    "USING wine_load\n",
    "   ON  hash(WINES.PURPLE_PIG.origin,WINES.PURPLE_PIG.section, WINES.PURPLE_PIG.name, WINES.PURPLE_PIG.description) = \\\n",
    "    hash(wine_load.origin,wine_load.section, wine_load.name, wine_load.description)\n",
    " WHEN NOT MATCHED THEN\n",
    " \t  INSERT (origin, section, name, description) VALUES (origin, section, name, description)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98aa020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                                                                                                                                                                                                                                                                     |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                            |userMetadata|\n",
      "+-------+-----------------------+------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "|1      |2022-06-10 13:31:07.067|null  |null    |MERGE    |[predicate -> (hash(spark_catalog.WINES.PURPLE_PIG.`origin`, spark_catalog.WINES.PURPLE_PIG.`section`, spark_catalog.WINES.PURPLE_PIG.`name`, spark_catalog.WINES.PURPLE_PIG.`description`) = hash(wine_load.`origin`, wine_load.`section`, wine_load.`name`, wine_load.`description`))]|null|null    |null     |0          |null          |false        |[numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 1, numTargetRowsInserted -> 0, numTargetRowsUpdated -> 0, numOutputRows -> 0, numSourceRows -> 544, numTargetFilesRemoved -> 0]|null        |\n",
      "|0      |2022-06-10 11:17:28.066|null  |null    |WRITE    |[mode -> ErrorIfExists, partitionBy -> []]                                                                                                                                                                                                                                              |null|null    |null     |null       |null          |true         |[numFiles -> 1, numOutputBytes -> 27903, numOutputRows -> 544]                                                                                                                                              |null        |\n",
      "+-------+-----------------------+------+--------+---------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, './tmp/delta')\n",
    "\n",
    "fullHistoryDF = deltaTable.history() \n",
    "\n",
    "fullHistoryDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35957f2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='wines', database='default', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='load_worked_hours', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='wine_load', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.catalog.listTables(\"default\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0eea478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/gary/WebScrapingIntoDelta/spark-warehouse'),\n",
       " Database(name='wines', description='', locationUri='file:/home/gary/WebScrapingIntoDelta/spark-warehouse/wines.db')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c48183eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
